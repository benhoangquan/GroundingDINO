{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgroundingdino\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minference\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_model\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mUniPose\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m inference_on_a_image \u001b[38;5;28;01mas\u001b[39;00m unipose\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Ignore warnings\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "from groundingdino.util.inference import load_model\n",
    "from ..UniPose import inference_on_a_image as unipose\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Global variables\n",
    "UNIPOSE_MODEL = unipose.load_model(\"../UniPose/config_model/UniPose_SwinT.py\", \"../UniPose/weights/unipose_swint.pth\", cpu_only=False)\n",
    "DINO_MODEL = load_model(\"groundingdino/config/GroundingDINO_SwinT_OGC.py\", \"weights/groundingdino_swint_ogc.pth\")\n",
    "UNIPOSE_CONFIG = {'box_threshold': 0.1, 'iou_threshold': 0.9}\n",
    "\n",
    "OUTPUT_PATH = \"../coco_dataset/test_grounding_dino\"\n",
    "INPUT_PATH = \"../coco_dataset/imgs_a\"\n",
    "TEXT_PROMPT = \"character . \"\n",
    "BOX_TRESHOLD = 0.35\n",
    "TEXT_TRESHOLD = 0.25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List and load images\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "def list_images(dir_path):\n",
    "    # List all files in directory recursively\n",
    "    return [y for x in os.walk(dir_path) for y in glob(os.path.join(x[0], '*.jpg'))] + \\\n",
    "           [y for x in os.walk(dir_path) for y in glob(os.path.join(x[0], '*.jpeg'))] + \\\n",
    "           [y for x in os.walk(dir_path) for y in glob(os.path.join(x[0], '*.png'))]\n",
    "\n",
    "from groundingdino.util.inference import load_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groundingdino.util.inference import predict\n",
    "\n",
    "# use grounding dino to extract boxes coordinates\n",
    "def detect_character(image):\n",
    "    boxes, logits, phrases = predict(\n",
    "        model=DINO_MODEL,\n",
    "        image=image,\n",
    "        caption=TEXT_PROMPT,\n",
    "        box_threshold=BOX_TRESHOLD,\n",
    "        text_threshold=TEXT_TRESHOLD\n",
    "    )\n",
    "    return boxes, logits, phrases\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.ops.boxes import box_convert\n",
    "import numpy as np\n",
    "\n",
    "def process_boxes(boxes, image_source):\n",
    "    h, w, _ = image_source.shape\n",
    "    boxes = boxes * torch.Tensor([w, h, w, h])\n",
    "    xyxy = box_convert(boxes=boxes, in_fmt=\"cxcywh\", out_fmt=\"xyxy\").numpy()\n",
    "    xyxy = [list(map(int, sublist)) for sublist in xyxy.tolist()]\n",
    "    return xyxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "def crop_box(image, box):\n",
    "    cropped = image[box[1]:box[3], box[0]:box[2], :]\n",
    "    return cv2.cvtColor(cropped, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "def crop_boxes(image, boxes):\n",
    "    return [crop_box(image, box) for box in boxes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ..UniPose import unipose_to_json as unipose_json\n",
    "\n",
    "\n",
    "def predict_unipose(image):\n",
    "    keypoints, _ = unipose_json.detect_keypoints(image, UNIPOSE_MODEL, UNIPOSE_CONFIG)\n",
    "    keypoints, _, _ = unipose_json.process_keypoints(keypoints, )\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from groundingdino.util.inference import annotate\n",
    "import cv2\n",
    "\n",
    "image_source, image = load_image(\".asset/cat_dog.jpeg\")\n",
    "boxes, logits, phrases = detect_character(image)\n",
    "box1 = process_boxes(boxes, image_source)[0]\n",
    "box2 = process_boxes(boxes, image_source)[1]\n",
    "img1 = crop_box(image_source, box1)\n",
    "img2 = crop_box(image_source, box2)\n",
    "cv2.imwrite(\"box1.jpg\", img1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
